# Open-RSBF

## Introduction & Motivation

This project explores employing budget forcing during GRPO training. The goal is to play around the trajectories sampling strategy in GRPO and see if we could improve. In particular, we force the trajectories with wrong answer to continue generate using a budget forcing like and append the "Wait" token. This is also similar to a close loop control and the "Wait" token is like a negative signal. 
We could argue if budget forcing works, the model could learn from trajectories that recover from mistakes and improve the self-correction ability.

## Inspiration

This work is inspired by the following paper(s):

* **s1: Simple test-time scaling**. [[arvix]](https://arxiv.org/abs/2501.19393) 
    * s1 show that using a simple technique called "budget forcing" could enhance the performance of a model. The technique is rather simple: append a token such as "Wait" to the generated text and force the model to continue generate. They show empirical results that this could enable the model to reflect on the generated text and correct itself even when the previous generation is wrong. 
* **scale up and distill down**
    * This paper scales up to a diverse dataset that contains both "perfect" trajectories and trajectories that fail the task at beginning but recover from the failure state. They observe including the latter trajectories enable the policy model to complete the task even when the initial trail is failed.
* **the deepseek r1 paper**.
    * Hyped paper with RLVR and GRPO, no need to talk about it.


## Implementation

We take the codebase from [open-rs]() which implemented with TRL. We modify the "Trainer" class in TRL and sample multiple rounds for each prompt. While the initial intention is to force continue sample only the failed trajectories, this introduce asymmetry in the rollout process. We end up sampling all the trajectories and then filter trajectories of each prompt for the highest reward(length penalty reward included).

## Results

The result show more stable training process in comparison to the baseline in open-rs, which trains with the standard GRPO. In particular, we do not observe a drastic performance drop in evaluation after a certain number of training steps as the baseline. The evaluation logs could be found in results/ folder.

## Thoughts & Discussion

* **On-policy RL**: Recent work on RL in LLMs mainly focuses on on-policy RL such as GRPO. This paper from [x]() shows that GRPO improve the sampling efficiency by reinforcing the success trajectories generated by the base model but does not gain the model the reasoning ability. This behavior is expected and could be explained in two perspectives:
    * Prior: the pretrained model is already strong prior generating sequences of text so strong that the model stuck in the local optima. There is also no explore-exploit tradeoff in the GRPO.
    * Action space support: while in classical RL, the policy model is initialized with a random policy which have the full support of the action space, in RL in foundation models, the policy model is initialized with a pretrained model which do not have the full action space.

* **Completion Length**: While some works observe better performance with longer completion length, other argued that the completion length does not help. In our method, ideally, the model should generate short completions for easier tasks and longer completions for harder tasks, where it might fail and reflect on itself.

* **Dense and Token-wise Rewards**: RLVR is known for its problem of scaled reward signal which is unable to validate the reasoning trace compared to methods that explicitly train a reward model. By rolling out multiple rounds, we could evaluate the reasoning trace at each round by its reward which potentially provides a more dense reward signal to the model. Also, we could implement a token-wise reward similar to DAPO and DR. GRPO to further improve the performance.

* **Multi-round rollout**: Recentlly there are many works on multi-round rollout or rejection sampling.  There are also works that mix SFT trajectories with the on-policy trajectories and train with GRPO.  

* **Why not further work on this?**: 
    * Model Size/ Compute: As I tried to make a minimal workable sample, I trained the 1.5B model. The model is not strong enough to show reflection ability. Besides, the implementation is based on TRL which is not scalable to larger models.
    * Other works: As mentioned above, there are already many works on multi-round rollout and rejection sampling coming out in recent weeks. They implemented with the veRL framework which is much more scalable and efficient than TRL. Therefore, I do not think it is necessary to further work on this.